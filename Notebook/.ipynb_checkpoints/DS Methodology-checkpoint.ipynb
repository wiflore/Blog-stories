{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Business Understanding\n",
    "Data Understanding\n",
    "Prepare Data\n",
    "Model Data\n",
    "Results\n",
    "Deploy\n",
    "\n",
    "https://docs.microsoft.com/es-es/azure/machine-learning/team-data-science-process/overview\n",
    "\n",
    "https://cloud.google.com/ml-engine/docs/tensorflow/ml-solutions-overview\n",
    "\n",
    "https://docs.microsoft.com/en-gb/azure/machine-learning/team-data-science-process/lifecycle#next-steps\n",
    "\n",
    "Data Analysis Process\n",
    "\n",
    "Extract - Obtain the data from a spreadsheet, SQL, the web, etc.  \n",
    "Clean - Here we could use exploratory visuals.  \n",
    "Explore - Here we use exploratory visuals.  \n",
    "Analyze - Here we might use either exploratory or explanatory visuals.  \n",
    "\n",
    "Visualization  \n",
    "Identifyint data types  \n",
    "Data Ink Ratio  \n",
    "Color  \n",
    "Shape, size .  \n",
    "\n",
    "Univariate  \n",
    "Tidy Data  \n",
    "Bar Charts  \n",
    "Absolute vs Relative Frequency  \n",
    "Counting Missing Data  \n",
    "Pie Charts Histograms Subplots Descriptive Stats, Outliers  \n",
    "Scales and Tranformations  \n",
    "Kernel Density Estimation  \n",
    "Waffle Plots  \n",
    "\n",
    "Bivariate  \n",
    "Scatterplots and Correlations  \n",
    "Overplotting, Transparency and Jitter  \n",
    "Heat Maps  \n",
    "Violing Plots  \n",
    "Box Plots  \n",
    "Clustered Bar  \n",
    "Categorical Plot  \n",
    "Faceting  \n",
    "Adaptation of Univariate  \n",
    "Line Plots  \n",
    "Q-Q Plots  \n",
    "Swarm plots  \n",
    "Rug and Strip Plots  \n",
    "Stacked Plots  \n",
    "Ridgeline Plots  \n",
    "  \n",
    "Mutivariate  \n",
    "Non-Positional Encodings shape, size, color  \n",
    "Color Palettes  \n",
    "Faceing in two Directions  \n",
    "Plot Matrices correlation  \n",
    "Feactures Engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "Now, we go through the lengthy process of preparing our data. We go through the following phases:  \n",
    "\n",
    "a) Merge  \n",
    "\n",
    "b) Missing Values Columms  \n",
    "\n",
    "c) Remove Uneeded Columns  \n",
    "\n",
    "c) Conversion  \n",
    "\n",
    "d) Missing Values Rows  \n",
    "\n",
    "e) Dummy variables  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "We now have data that is cleaned and not missing values. But let's break our modeling to different phases:  \n",
    "\n",
    "a) EDA  \n",
    "\n",
    "b) Base Model  \n",
    "\n",
    "c) Feature Selection  \n",
    "\n",
    "d) Improved model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment and Testing\n",
    "We will deploy the model and test it on New York data. But if our assumption is that our model is general, we want to do the following:  \n",
    "\n",
    "Use the most important features. This is to deploy a fast model.  \n",
    "\n",
    "Remove host diff since it is asking for a variable found in listing which is a huge csv for New York for example`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Understanding:** This data contain a lot of parameters related to Boston and Seattle Airbnb data. Airbnb is in the business of offering host locations for cutomers. Like Amazon, those hosts rely on price and review score. I am assuming that review score correlates with profitability.\n",
    "\n",
    "**Data Understanding:** The data is very dirty but contain a lot of informative insights. So we are required to do a lot of data preprocessing with the data.\n",
    "\n",
    "**Data cleaning and preprocessing:** I spent a lot of time on cleaning the data and you can see from the code that the majority of my work is on data processing. At first, I started by removing columns that are not needed to trim the matrix, then I started remvoing missing data, then removing null data, and then creating dummy variables for categorical columns.\n",
    "\n",
    "**Analysis:** I created a heatmap to show the correlation of data and removed data that are strongly correlated with each other.\n",
    "\n",
    "**Model:** I used a base model which is linear regression which performed poorly. Then, I used decision tree regressor. The model score is 99%.\n",
    "\n",
    "**Deploy:** To deploy this model quickly, we need to neglict calender list since it is huge and rely on the 9 features that we have except host diff. Our model still give 99% accuracy for Boston and Seattle but does not give good results for New York and San Franscisco indicating that each city need its own model and that we need to aggregate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
